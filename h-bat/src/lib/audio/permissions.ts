'use client'

import * as Tone from 'tone'

// éŸ³éŸ¿æ¨©é™ã®çŠ¶æ…‹
export interface AudioPermissionState {
  hasUserGesture: boolean
  contextState: AudioContextState
  autoplayAllowed: boolean
  lastUserInteraction: Date | null
  permissionDenied: boolean
  error: string | null
}

// ãƒ¦ãƒ¼ã‚¶ãƒ¼æ“ä½œã®ç¨®é¡
export type UserGestureType = 'click' | 'touch' | 'keypress' | 'mousedown' | 'touchstart'

// éŸ³éŸ¿æ¨©é™ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
class AudioPermissionManager {
  private state: AudioPermissionState = {
    hasUserGesture: false,
    contextState: 'suspended',
    autoplayAllowed: false,
    lastUserInteraction: null,
    permissionDenied: false,
    error: null
  }

  private gestureEventTypes: UserGestureType[] = ['click', 'touch', 'keypress', 'mousedown', 'touchstart']
  private gestureListeners: Map<UserGestureType, (event: Event) => void> = new Map()
  private isListening = false

  constructor() {
    this.updateContextState()
    this.setupAutoplayDetection()
  }

  // éŸ³éŸ¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®çŠ¶æ…‹ã‚’æ›´æ–°
  private updateContextState(): void {
    if (typeof window !== 'undefined' && Tone.context) {
      this.state.contextState = Tone.context.state
    }
  }

  // è‡ªå‹•å†ç”Ÿã®æ¤œå‡º
  private async setupAutoplayDetection(): Promise<void> {
    if (typeof window === 'undefined') return

    try {
      // å°ã•ãªç„¡éŸ³ãƒ•ã‚¡ã‚¤ãƒ«ã§è‡ªå‹•å†ç”Ÿãƒ†ã‚¹ãƒˆ
      const testAudio = new Audio()
      testAudio.volume = 0
      testAudio.muted = true
      
      const playPromise = testAudio.play()
      
      if (playPromise) {
        await playPromise
        this.state.autoplayAllowed = true
        console.log('âœ… Autoplay is allowed')
      }
    } catch (error) {
      this.state.autoplayAllowed = false
      console.log('âš ï¸ Autoplay is blocked')
    }
  }

  // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã®ãƒªã‚¹ãƒŠãƒ¼ã‚’è¨­å®š
  startListeningForUserGesture(): void {
    if (this.isListening || typeof window === 'undefined') return

    this.gestureEventTypes.forEach(eventType => {
      const listener = (event: Event) => this.handleUserGesture(event, eventType)
      this.gestureListeners.set(eventType, listener)
      
      // ãƒ‘ãƒƒã‚·ãƒ–ãƒªã‚¹ãƒŠãƒ¼ã§è¨­å®šï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šï¼‰
      document.addEventListener(eventType, listener, { 
        passive: true, 
        once: false // è¤‡æ•°å›ã®æ“ä½œã«å¯¾å¿œ
      })
    })

    this.isListening = true
    console.log('ğŸ‘‚ Listening for user gestures...')
  }

  // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã®ãƒªã‚¹ãƒŠãƒ¼ã‚’åœæ­¢
  stopListeningForUserGesture(): void {
    if (!this.isListening) return

    this.gestureEventTypes.forEach(eventType => {
      const listener = this.gestureListeners.get(eventType)
      if (listener) {
        document.removeEventListener(eventType, listener)
      }
    })

    this.gestureListeners.clear()
    this.isListening = false
    console.log('ğŸ”‡ Stopped listening for user gestures')
  }

  // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã®å‡¦ç†
  private async handleUserGesture(event: Event, gestureType: UserGestureType): Promise<void> {
    console.log(`ğŸ‘† User gesture detected: ${gestureType}`)
    
    this.state.lastUserInteraction = new Date()
    this.state.hasUserGesture = true
    this.state.error = null

    // éŸ³éŸ¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®é–‹å§‹ã‚’è©¦è¡Œ
    try {
      await this.attemptAudioStart()
    } catch (error) {
      console.error('Failed to start audio after user gesture:', error)
      this.state.error = error instanceof Error ? error.message : 'Unknown error'
    }
  }

  // éŸ³éŸ¿é–‹å§‹ã®è©¦è¡Œ
  private async attemptAudioStart(): Promise<boolean> {
    try {
      if (Tone.context.state !== 'running') {
        await Tone.start()
        console.log('ğŸµ Audio context started successfully')
      }

      this.updateContextState()
      return true
    } catch (error) {
      console.error('âŒ Failed to start audio context:', error)
      this.state.permissionDenied = true
      this.state.error = error instanceof Error ? error.message : 'Audio start failed'
      return false
    }
  }

  // æ‰‹å‹•ã§ã®éŸ³éŸ¿é–‹å§‹è¦æ±‚
  async requestAudioPermission(): Promise<boolean> {
    console.log('ğŸµ Requesting audio permission...')

    if (!this.state.hasUserGesture) {
      console.warn('âš ï¸ No user gesture detected. Audio start may fail.')
    }

    try {
      const success = await this.attemptAudioStart()
      
      if (success) {
        console.log('âœ… Audio permission granted')
        return true
      } else {
        console.error('âŒ Audio permission denied')
        return false
      }
    } catch (error) {
      console.error('âŒ Error requesting audio permission:', error)
      this.state.error = error instanceof Error ? error.message : 'Permission request failed'
      return false
    }
  }

  // éŸ³éŸ¿æ¨©é™ã®çŠ¶æ…‹å–å¾—
  getPermissionState(): AudioPermissionState {
    this.updateContextState()
    return { ...this.state }
  }

  // éŸ³éŸ¿å†ç”Ÿã®æº–å‚™çŠ¶æ³ãƒã‚§ãƒƒã‚¯
  isReadyForAudio(): boolean {
    return (
      this.state.hasUserGesture &&
      this.state.contextState === 'running' &&
      !this.state.permissionDenied
    )
  }

  // ãƒ¦ãƒ¼ã‚¶ãƒ¼æ“ä½œãªã—ã§ã®éŸ³éŸ¿ãƒ†ã‚¹ãƒˆï¼ˆé–‹ç™ºç”¨ï¼‰
  async testAudioWithoutGesture(): Promise<{
    canStart: boolean
    contextState: AudioContextState
    error?: string
  }> {
    try {
      await Tone.start()
      this.updateContextState()
      
      return {
        canStart: true,
        contextState: this.state.contextState
      }
    } catch (error) {
      return {
        canStart: false,
        contextState: this.state.contextState,
        error: error instanceof Error ? error.message : 'Test failed'
      }
    }
  }

  // ãƒ–ãƒ©ã‚¦ã‚¶ã®éŸ³éŸ¿ã‚µãƒãƒ¼ãƒˆè©³ç´°ãƒã‚§ãƒƒã‚¯
  checkBrowserAudioSupport(): {
    hasWebAudio: boolean
    hasAudioContext: boolean
    hasMediaDevices: boolean
    supportedCodecs: string[]
    browserInfo: {
      userAgent: string
      isMobile: boolean
      isIOS: boolean
      isAndroid: boolean
      isChrome: boolean
      isSafari: boolean
      isFirefox: boolean
    }
  } {
    const userAgent = navigator.userAgent
    
    return {
      hasWebAudio: 'AudioContext' in window || 'webkitAudioContext' in window,
      hasAudioContext: !!window.AudioContext || !!(window as any).webkitAudioContext,
      hasMediaDevices: 'mediaDevices' in navigator,
      supportedCodecs: this.getSupportedAudioCodecs(),
      browserInfo: {
        userAgent,
        isMobile: /Mobi|Android/i.test(userAgent),
        isIOS: /iPad|iPhone|iPod/.test(userAgent),
        isAndroid: /Android/.test(userAgent),
        isChrome: /Chrome/.test(userAgent),
        isSafari: /Safari/.test(userAgent) && !/Chrome/.test(userAgent),
        isFirefox: /Firefox/.test(userAgent)
      }
    }
  }

  // ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹éŸ³éŸ¿ã‚³ãƒ¼ãƒ‡ãƒƒã‚¯ã®å–å¾—
  private getSupportedAudioCodecs(): string[] {
    const audio = document.createElement('audio')
    const codecs = [
      { name: 'WAV', type: 'audio/wav' },
      { name: 'MP3', type: 'audio/mpeg' },
      { name: 'OGG', type: 'audio/ogg' },
      { name: 'WEBM', type: 'audio/webm' },
      { name: 'AAC', type: 'audio/mp4; codecs="mp4a.40.2"' },
      { name: 'FLAC', type: 'audio/flac' }
    ]

    return codecs
      .filter(codec => audio.canPlayType(codec.type) !== '')
      .map(codec => codec.name)
  }

  // æ¨©é™çŠ¶æ…‹ã®ãƒªã‚»ãƒƒãƒˆ
  reset(): void {
    this.state = {
      hasUserGesture: false,
      contextState: 'suspended',
      autoplayAllowed: false,
      lastUserInteraction: null,
      permissionDenied: false,
      error: null
    }
    
    this.stopListeningForUserGesture()
    console.log('ğŸ”„ Audio permission state reset')
  }

  // éŸ³éŸ¿æ¨©é™ã®è¨ºæ–­
  diagnoseAudioIssues(): {
    issues: string[]
    recommendations: string[]
    severity: 'low' | 'medium' | 'high'
  } {
    const issues: string[] = []
    const recommendations: string[] = []
    
    const state = this.getPermissionState()
    const support = this.checkBrowserAudioSupport()

    // åŸºæœ¬çš„ãªå•é¡Œã®ãƒã‚§ãƒƒã‚¯
    if (!support.hasWebAudio) {
      issues.push('Web Audio API not supported')
      recommendations.push('Use a modern browser that supports Web Audio API')
    }

    if (!state.hasUserGesture) {
      issues.push('No user interaction detected')
      recommendations.push('Click or tap anywhere to enable audio')
    }

    if (state.contextState !== 'running') {
      issues.push(`Audio context is ${state.contextState}`)
      recommendations.push('User interaction is required to start audio')
    }

    if (!state.autoplayAllowed) {
      issues.push('Autoplay is blocked by browser')
      recommendations.push('User interaction is required before playing audio')
    }

    if (state.permissionDenied) {
      issues.push('Audio permission was denied')
      recommendations.push('Check browser audio settings and reload the page')
    }

    // ãƒ–ãƒ©ã‚¦ã‚¶å›ºæœ‰ã®å•é¡Œ
    if (support.browserInfo.isIOS) {
      issues.push('iOS requires user interaction for audio')
      recommendations.push('Tap the screen before starting audio tests')
    }

    if (support.browserInfo.isSafari) {
      issues.push('Safari has strict autoplay policies')
      recommendations.push('Use user interaction before playing audio')
    }

    // é‡è¦åº¦ã®åˆ¤å®š
    let severity: 'low' | 'medium' | 'high' = 'low'
    if (!support.hasWebAudio || state.permissionDenied) {
      severity = 'high'
    } else if (!state.hasUserGesture || state.contextState !== 'running') {
      severity = 'medium'
    }

    return { issues, recommendations, severity }
  }
}

// ã‚°ãƒ­ãƒ¼ãƒãƒ«éŸ³éŸ¿æ¨©é™ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
export const audioPermissionManager = new AudioPermissionManager()

// ä¾¿åˆ©ãªé–¢æ•°ç¾¤
export async function requestAudioPermission(): Promise<boolean> {
  return await audioPermissionManager.requestAudioPermission()
}

export function startListeningForUserGesture(): void {
  audioPermissionManager.startListeningForUserGesture()
}

export function stopListeningForUserGesture(): void {
  audioPermissionManager.stopListeningForUserGesture()
}

export function getAudioPermissionState(): AudioPermissionState {
  return audioPermissionManager.getPermissionState()
}

export function isAudioReady(): boolean {
  return audioPermissionManager.isReadyForAudio()
}

export function checkBrowserAudioSupport() {
  return audioPermissionManager.checkBrowserAudioSupport()
}

export function diagnoseAudioIssues() {
  return audioPermissionManager.diagnoseAudioIssues()
}

// H-BATç‰¹åŒ–ã®éŸ³éŸ¿æ¨©é™ãƒã‚§ãƒƒã‚¯
export async function ensureHBatAudioReady(): Promise<{
  isReady: boolean
  issues: string[]
  recommendations: string[]
}> {
  console.log('ğŸµ Checking H-BAT audio readiness...')
  
  const permissionState = getAudioPermissionState()
  const diagnosis = diagnoseAudioIssues()
  
  // H-BATç‰¹æœ‰ã®è¦ä»¶ãƒã‚§ãƒƒã‚¯
  const hbatIssues = [...diagnosis.issues]
  const hbatRecommendations = [...diagnosis.recommendations]
  
  // ç²¾å¯†ãªéŸ³éŸ¿æ¸¬å®šã®ãŸã‚ã®è¿½åŠ ãƒã‚§ãƒƒã‚¯
  if (permissionState.contextState === 'running') {
    const sampleRate = Tone.context.sampleRate
    if (sampleRate < 44100) {
      hbatIssues.push(`Low sample rate: ${sampleRate}Hz`)
      hbatRecommendations.push('Use a device with 44.1kHz or higher sample rate for accurate measurements')
    }
  }

  const isReady = (
    permissionState.hasUserGesture &&
    permissionState.contextState === 'running' &&
    !permissionState.permissionDenied &&
    hbatIssues.length === 0
  )

  if (isReady) {
    console.log('âœ… H-BAT audio is ready')
  } else {
    console.warn('âš ï¸ H-BAT audio not ready:', hbatIssues)
  }

  return {
    isReady,
    issues: hbatIssues,
    recommendations: hbatRecommendations
  }
} 